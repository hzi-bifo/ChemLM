{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9885064,"sourceType":"datasetVersion","datasetId":6070162},{"sourceId":9885529,"sourceType":"datasetVersion","datasetId":6070524},{"sourceId":9893043,"sourceType":"datasetVersion","datasetId":6076119}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Script for ChemBERTa evaluation. Add the data path of the script and the dataset name for evaluation. ","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nimport numpy as np\nimport torch\nfrom sklearn import metrics\nfrom transformers import EarlyStoppingCallback\nimport random\nfrom transformers import set_seed\nseed =14","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T14:50:28.409654Z","iopub.execute_input":"2024-11-14T14:50:28.410132Z","iopub.status.idle":"2024-11-14T14:50:55.345706Z","shell.execute_reply.started":"2024-11-14T14:50:28.410052Z","shell.execute_reply":"2024-11-14T14:50:55.344746Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"data_path = '.../pr/ChemLM/data/benchmark' #add data path here\ndataset = 'clintox'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nrandom.seed(seed)\nset_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T14:50:55.347322Z","iopub.execute_input":"2024-11-14T14:50:55.348191Z","iopub.status.idle":"2024-11-14T14:50:55.361247Z","shell.execute_reply.started":"2024-11-14T14:50:55.348154Z","shell.execute_reply":"2024-11-14T14:50:55.360453Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    # Unpack predictions and labels from eval_pred\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    auc = metrics.roc_auc_score(labels,predictions, average='macro')\n    f1=metrics.f1_score(labels,predictions, average='macro')\n    precision=metrics.precision_score(labels,predictions,average='macro')\n    recall=metrics.recall_score(labels,predictions,average='macro')\n    acc=metrics.accuracy_score(labels,predictions)\n\n    f1_binary=metrics.f1_score(labels,predictions,pos_label=1, average='binary')\n    precision_binary=metrics.precision_score(labels,predictions,pos_label=1,average='binary')\n    recall_binary=metrics.recall_score(labels,predictions,pos_label=1, average='binary')\n\n\n    # Return a dictionary of metrics\n    return {\n        'accuracy': acc,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'f1_pos': f1_binary,\n        'auc': auc\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T14:50:55.362463Z","iopub.execute_input":"2024-11-14T14:50:55.363181Z","iopub.status.idle":"2024-11-14T14:50:55.438456Z","shell.execute_reply.started":"2024-11-14T14:50:55.363136Z","shell.execute_reply":"2024-11-14T14:50:55.437673Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, RobertaForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_180k\")\nmodel = RobertaForSequenceClassification.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_180k\", num_labels=2)\n\n# tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n# model = RobertaForSequenceClassification.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\", num_labels=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndef load_data(path):\n  df = pd.read_csv(path, sep = '\\t')\n  print(df)\n  return df\n\nif dataset == 'bace':\n    train_data = load_data(f'{data_path}/train_bace_clf_stratified_0.2_clf.csv')\n    valid_data = load_data(f'{data_path}/validation_bace_clf_stratified_0.2_clf.csv')\n    test_data = load_data(f'{data_path}/test_bace_clf_stratified_0.2_clf.csv')\nelse:    \n    train_data = load_data(f'{data_path}/train_{dataset}_stratified_0.2_clf.csv')\n    valid_data = load_data(f'{data_path}/validation_{dataset}_stratified_0.2_clf.csv')\n    test_data = load_data(f'{data_path}/test_{dataset}_stratified_0.2_clf.csv')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FinetuneDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer, include_labels=True):\n        self.encodings = tokenizer(df[\"smiles\"].tolist(), truncation=True, padding=True)\n        self.labels = df.iloc[:, 1].values\n        self.include_labels = include_labels\n\n    def __getitem__(self, idx):\n        #item = {\"input_ids\": self.encodings[\"input_ids\"]}\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.include_labels and self.labels is not None:\n            item[\"labels\"] = torch.tensor(int(self.labels[idx]))\n        return item\n\n    def check(self):\n        item = {key: torch.tensor(val[0]) for key, val in self.encodings.items()}\n        if self.include_labels and self.labels is not None:\n            item[\"labels\"] = torch.tensor(int(self.labels[0]))\n        return item\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\ntrain_dataset = FinetuneDataset(train_data, tokenizer)\nvalid_dataset = FinetuneDataset(valid_data, tokenizer)\ntest_dataset = FinetuneDataset(test_data, tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = FinetuneDataset(train_data, tokenizer)\nvalid_dataset = FinetuneDataset(valid_data, tokenizer)\ntest_dataset = FinetuneDataset(test_data, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T14:50:59.424877Z","iopub.execute_input":"2024-11-14T14:50:59.425209Z","iopub.status.idle":"2024-11-14T14:50:59.570620Z","shell.execute_reply.started":"2024-11-14T14:50:59.425169Z","shell.execute_reply":"2024-11-14T14:50:59.569819Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"training_args = TrainingArguments(\n        evaluation_strategy='epoch',\n        learning_rate= 5e-4,\n        save_strategy = 'epoch',\n        eval_steps = 1000,\n        logging_steps=100,\n        #save_steps=10000,\n        load_best_model_at_end=True,\n        output_dir='/content/',\n        overwrite_output_dir=True,\n        num_train_epochs= 100,\n        per_device_train_batch_size=64,\n        per_device_eval_batch_size= 64,\n        fp16=torch.cuda.is_available(),  # fp16 only works on CUDA devices\n        report_to=\"none\"  # Disables W&B logging\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n            model,\n            training_args,\n            train_dataset=train_dataset,\n            eval_dataset=valid_dataset,\n            tokenizer=tokenizer,\n            compute_metrics=compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wandb disabled\nimport os\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n\ntrainer.train()\ntrainer.save_model(f'chemberta_{dataset}_f_check')\n# Evaluate on the test set\ntest_results = trainer.predict(test_dataset)\nprint(test_results.metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('ChemBERTa & {} & {} & {} & {} & {}'.format(round(test_results.metrics['test_f1'],2), round(test_results.metrics['test_auc'],2), round(test_results.metrics['test_precision'],2), round(test_results.metrics['test_recall'],2), round(test_results.metrics['test_accuracy'],2)) )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}